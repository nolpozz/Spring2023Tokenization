# Spring2023Tokenization
Expand knowledge of lower level NLP processes, specifically tokenization

This project uses various online tutorials and projects to explore the process of tokenization and the probabalistic and technological subfields that it engages with.

Rough Process

**
Finite state automata
non-determanistic
   more than one arrow from a given input at a given state
epsilon transitions


current methods for subword recognition and run them on multiple sets of data
bite pair encoding; sentence piece - algorithms for tokenization

read text
find english corpus and play around with BPE and Sentence Piece\

Finish Unigram
graph change of merges
research and think critically about algorithms and objectives

read paper1
look at differences between sentenece piece on github and unigram hugging face 

reproduce one of the paper's experiments 

Get n best from hugging face 
read kudo unigram paper
Learn a tokenizer and scores of best and nbest tokenizations 
look at differences between each of the nbest tokenizations
is nbest by word or by corpus
**


Sources
https://huggingface.co/learn/nlp-course/chapter2/4?fw=tf
https://github.com/huggingface/tokenizers
https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt
https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt
https://github.com/google/sentencepiece

Papers

Subword Regularization: Improving Neural Network Translation Models
with Multiple Subword Candidates
Taku Kudo

You should evaluate your language model on marginal likelihood over
tokenisations
Kris Cao and Laura Rimell

Thanks
Kartik Goyal
https://www.ttic.edu/faculty/goyal/
